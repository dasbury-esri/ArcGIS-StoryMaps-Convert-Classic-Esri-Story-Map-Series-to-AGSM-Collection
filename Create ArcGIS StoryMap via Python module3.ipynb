{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e454d37a",
   "metadata": {},
   "source": [
    "# Generalized sketch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70cb43f",
   "metadata": {},
   "source": [
    "TO DO - create conversion tool for classic swipe (second tab in Katrina story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bc65aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "from arcgis.apps.storymap import StoryMap, Themes, Image, Video, Audio, Embed, Map, Button, Text, Gallery, Timeline, Sidecar, Code, Table, TextStyles\n",
    "from arcgis.gis import GIS, Item\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import re, json, requests, sys, time \n",
    "\n",
    "agoNotebook = False\n",
    "\n",
    "# Set Pandas dataframe display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9224ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.11.11 (main, Mar  3 2025, 15:29:37) [MSC v.1938 64 bit (AMD64)]\n",
      "ArcGIS for Python API / StoryMap module version:  2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Print Python and ArcGIS for Python versions\n",
    "# since things can change between versions\n",
    "import sys\n",
    "print(f\"Python version: \",sys.version)\n",
    "import arcgis\n",
    "print(\"ArcGIS for Python API / StoryMap module version: \",arcgis.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6563a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in as: dasbury_storymaps (role: org_admin)\n"
     ]
    }
   ],
   "source": [
    "# Connect to ArcGIS Online\n",
    "# Define the GIS\n",
    "if agoNotebook == False:\n",
    "    import keyring\n",
    "    service_name = \"system\" # Use the default local credential store\n",
    "    success = False # Set initial state\n",
    "\n",
    "    # Ask for the username\n",
    "    while success == False:\n",
    "        username_for_keyring = input(\"Enter your ArcGIS Online username:\") # If you are using VS Code, the text input dialog box appears at the top of the window\n",
    "        # Get the credential object\n",
    "        credential = keyring.get_credential(service_name, username_for_keyring)\n",
    "        # Check if the username is in the credential store\n",
    "        if credential is None:\n",
    "            print(f\"'{username_for_keyring}' is not in the local system's credential store. Try another username.\")\n",
    "        # Retrieve the password, login and set the GIS portal\n",
    "        else:\n",
    "            password_from_keyring = keyring.get_password(\"system\", username_for_keyring)\n",
    "            portal_url = 'https://www.arcgis.com'  \n",
    "            gis = GIS(portal_url, username=username_for_keyring, password=password_from_keyring)\n",
    "            success = True\n",
    "            # Print a success message with username and user's organization role\n",
    "            print(\"Successfully logged in as: \" + gis.properties.user.username, \"(role: \" + gis.properties.user.role + \")\")\n",
    "else:\n",
    "    gis = GIS(\"home\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "051258d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Classic StoryMap item id\n",
    "classic_storymap_id = '597d573e58514bdbbeb53ba2179d2359'\n",
    "# Fetch the StoryMap Item from AGO\n",
    "classic_item = Item(gis=gis,itemid=classic_storymap_id)\n",
    "# Fetch the StoryMap data\n",
    "classic_data = Item.get_data(classic_item)\n",
    "if type(classic_data) == dict:\n",
    "    classic_item_json = json.dumps(classic_data)\n",
    "    classic_item_data = json.loads(classic_item_json)\n",
    "else:\n",
    "    classic_item_data = json.loads(classic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a736a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def convert_color_style_to_class(tag):\n",
    "    # Check if tag has 'style' attribute with color\n",
    "    style = tag.get('style', '')\n",
    "    # Regex to find color property (hex, rgb, named colors)\n",
    "    match = re.search(r'color\\s*:\\s*([^;]+)', style, re.IGNORECASE)\n",
    "    if match:\n",
    "        color_value = match.group(1).strip()\n",
    "        # Convert hex (#XXXXXX) to class name, removing #\n",
    "        if color_value.startswith('#'):\n",
    "            class_color = f\"sm-text-color-{color_value[1:].upper()}\"\n",
    "        else:\n",
    "            # For rgb or named color, sanitize usable string (replace spaces/paren)\n",
    "            sanitized = re.sub(r'[\\s\\(\\)]', '', color_value).replace(',', '-')\n",
    "            class_color = f\"sm-text-color-{sanitized.upper()}\"\n",
    "        # Remove color from style attribute\n",
    "        new_style = re.sub(r'color\\s*:\\s*[^;]+;?', '', style, flags=re.IGNORECASE).strip()\n",
    "        if new_style:\n",
    "            tag['style'] = new_style\n",
    "        else:\n",
    "            del tag['style']\n",
    "        # Add or append class attribute\n",
    "        if 'class' in tag.attrs:\n",
    "            tag['class'].append(class_color)\n",
    "        else:\n",
    "            tag['class'] = [class_color]\n",
    "\n",
    "def process_html_colors_preserve_html(html_text):\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    # Iterate over tags that can have styles: div, span, strong, em, p, etc.\n",
    "    for tag in soup.find_all(True):\n",
    "        convert_color_style_to_class(tag)\n",
    "    return str(soup)\n",
    "\n",
    "def parse_content_element(el):\n",
    "    tag_name = el.name\n",
    "    if tag_name == \"p\": # or tag_name in [\"span\", \"strong\", \"em\", \"div\"]:\n",
    "        # Extract inner HTML preserving inline styles\n",
    "        inner_html = ''.join(str(c) for c in el.contents)\n",
    "        processed_html = process_html_colors_preserve_html(inner_html)\n",
    "        return Text(text=processed_html, style=TextStyles.PARAGRAPH)\n",
    "    \n",
    "    elif tag_name == \"img\":\n",
    "        src = el.get(\"src\")\n",
    "        alt = el.get(\"alt\", \"\")\n",
    "        link = \"\"\n",
    "        if el.get(\"href\"):\n",
    "            link = el.get(\"href\")\n",
    "        img = Image(path=src)\n",
    "        img.alt_text = alt\n",
    "        img.caption = \"\" # TO DO try to find Classic stories that have images with captions\n",
    "        img.link = link\n",
    "        img.image = src  # Assign image property. TO DO fix this for images hosted on AGO\n",
    "        return img\n",
    "\n",
    "    elif tag_name == \"video\":\n",
    "        src = el.get(\"src\")\n",
    "        alt = el.get(\"alt\", \"\")\n",
    "        vid = Video(path=src)\n",
    "        vid.alt_text = alt\n",
    "        vid.caption = \"\" # TO DO try to find Classic stories that have Videos with captions\n",
    "        vid.video = src # Assign video property. TO DO fix this for hosted videos\n",
    "        return vid\n",
    "    \n",
    "    elif tag_name == \"audio\":\n",
    "        src = el.get(\"src\")\n",
    "        alt = el.get(\"alt\", \"\")\n",
    "        aud = Audio(path=src)\n",
    "        aud.alt_text = alt\n",
    "        aud.caption = \"\" # TO DO try to find Classic stories that have Audio with captions\n",
    "        aud.audio = src # Assign Audio property. TO DO fix this for hosted videos\n",
    "        return aud\n",
    "    \n",
    "    elif tag_name == \"iframe\" or tag_name == \"embed\":\n",
    "        src = el.get(\"src\") or el.get(\"data-src\")\n",
    "        alt = el.get(\"alt\", \"\")\n",
    "        if src:\n",
    "            emb = Embed(path=src)\n",
    "            emb.alt_text = alt\n",
    "            emb.caption = \"\" # TO DO try to find Classic stories that have Embeds with captions\n",
    "            emb.link = src\n",
    "        return emb\n",
    "\n",
    "    elif tag_name == \"map\":\n",
    "        src = el.get(\"src\")\n",
    "        alt = el.get(\"alt\", \"\")\n",
    "        extent = \"\" #TO DO get extent\n",
    "        layers = \"\" # TO DO get map layers\n",
    "        mp = Map(item=\"\")\n",
    "        mp.alt_text = alt\n",
    "        mp.caption = \"\" # TO DO try to find Classic stories that have Maps in Sidecar panel with captions\n",
    "        mp.map = src\n",
    "        mp.map_layers = layers \n",
    "        mp.set_viewpoint = extent\n",
    "        return aud\n",
    "    \n",
    "    else:\n",
    "        # Fallback for unsupported or unknown types - treat as text\n",
    "        inner_html = ''.join(str(c) for c in el.contents)\n",
    "        processed_html = process_html_colors_preserve_html(inner_html)\n",
    "        return Text(text=processed_html, style=TextStyles.PARAGRAPH)\n",
    "\n",
    "# def deduplicate_by_containment(elements):\n",
    "#     # Create list of (element, outer_html_str) tuples\n",
    "#     elems_and_html = [(el, ' '.join(str(el).split())) for el in elements]\n",
    "\n",
    "#     keep = []\n",
    "#     for i, (el_i, html_i) in enumerate(elems_and_html):\n",
    "#         # Check if this element is contained within another (excluding itself)\n",
    "#         contained = False\n",
    "#         for j, (el_j, html_j) in enumerate(elems_and_html):\n",
    "#             if i != j and html_i in html_j:\n",
    "#                 contained = True\n",
    "#                 break\n",
    "#         if not contained:\n",
    "#             keep.append(el_i)\n",
    "#     return keep\n",
    "\n",
    "# def parse_narrative_html(html_text):\n",
    "#     soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "#     content_nodes = []\n",
    "#     for child in soup.children:\n",
    "#         if isinstance(child, str):\n",
    "#             # Text node (likely whitespace) - skip or wrap in Text()\n",
    "#             node = Text(child)\n",
    "#             continue\n",
    "#         node = parse_content_element(child)\n",
    "#         print(type(node))\n",
    "#         if node:\n",
    "#             content_nodes.append(node)\n",
    "#     #deduped_nodes = deduplicate_by_containment(content_nodes)\n",
    "#     return content_nodes\n",
    "\n",
    "def parse_html_elements(html_snippet):\n",
    "    soup = BeautifulSoup(html_snippet, \"html.parser\")\n",
    "    soup_list = [child for child in soup.contents if getattr(child, 'name', None)]\n",
    "    html_elements = []\n",
    "    for element in soup_list:\n",
    "        for c in element:\n",
    "            if getattr(c, 'name', None):\n",
    "                html_elements.append(c)\n",
    "    return html_elements\n",
    "\n",
    "def convert_html_elements_to_storymap_content(html_elements):\n",
    "    content_nodes = []\n",
    "    for el in html_elements:\n",
    "        node = parse_content_element(el)\n",
    "        if node:\n",
    "            content_nodes.append(node)\n",
    "    return content_nodes\n",
    "\n",
    "def extract_deepest_relevant_blocks(html, relevant_tags=None):\n",
    "    if relevant_tags is None:\n",
    "        # Add or remove tags as needed for your use case\n",
    "        relevant_tags = [\n",
    "            {\"name\": \"img\"},\n",
    "            {\"name\": \"figure\"},\n",
    "            {\"name\": \"figure\", \"class_\": \"caption\"},\n",
    "            {\"name\": \"p\"},\n",
    "            {\"name\": \"div\", \"class_\": \"image-container\"}\n",
    "        ]\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    results = []\n",
    "\n",
    "    def is_relevant(tag):\n",
    "        for spec in relevant_tags:\n",
    "            if tag.name == spec[\"name\"]:\n",
    "                if \"class_\" in spec:\n",
    "                    if tag.get(\"class\") and spec[\"class_\"] in tag.get(\"class\"):\n",
    "                        return True\n",
    "                else:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def is_deepest_relevant(tag):\n",
    "        # Check if any child is also a relevant tag\n",
    "        for child in tag.find_all(True, recursive=False):\n",
    "            if is_relevant(child):\n",
    "                return False\n",
    "        return is_relevant(tag)\n",
    "\n",
    "    # Traverse all tags and collect only the deepest relevant ones\n",
    "    for tag in soup.find_all(True):\n",
    "        if is_deepest_relevant(tag):\n",
    "            results.append(str(tag).strip())\n",
    "\n",
    "    return results\n",
    "\n",
    "def extract_content_blocks(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Find the main container (if present)\n",
    "    main = soup.find(class_=\"description\") or soup\n",
    "    # Get all direct children that are tags (not NavigableString)\n",
    "    blocks = [child for child in main.children if getattr(child, 'name', None)]\n",
    "    # For each block, get its full HTML string\n",
    "    elements_list = []\n",
    "    for block in blocks:\n",
    "        html_str = str(block).strip()\n",
    "        # Skip empty blocks\n",
    "        if html_str and html_str != \"&nbsp;\":\n",
    "            elements_list.append(html_str)\n",
    "    return elements_list\n",
    "\n",
    "def split_nested_blocks(html, target_tags=None):\n",
    "    if target_tags is None:\n",
    "        target_tags = [\n",
    "            {\"name\": \"div\", \"class_\": \"image-container\"},\n",
    "            {\"name\": \"figure\"}\n",
    "        ]\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    results = []\n",
    "    # For each target tag, find all matching elements\n",
    "    for target in target_tags:\n",
    "        found = soup.find_all(target[\"name\"], class_=target.get(\"class_\"))\n",
    "        for el in found:\n",
    "            results.append(str(el).strip())\n",
    "    # If no target tags found, keep the original block\n",
    "    if not results:\n",
    "        results.append(html.strip())\n",
    "    return results\n",
    "\n",
    "def filter_parent_blocks(blocks, target_tags=None):\n",
    "    filtered = []\n",
    "    for block in blocks:\n",
    "        split_items = split_nested_blocks(block, target_tags)\n",
    "        # If split_items contains only the original block, keep it\n",
    "        if len(split_items) == 1 and split_items[0] == block.strip():\n",
    "            filtered.append(block.strip())\n",
    "        # If split_items contains only target elements, keep only those\n",
    "        elif len(split_items) > 1:\n",
    "            filtered.extend(split_items)\n",
    "    return filtered\n",
    "\n",
    "def filter_parent_blocks_strict(blocks, target_tags=None):\n",
    "    if target_tags is None:\n",
    "        target_tags = [\n",
    "            {\"name\": \"div\", \"class_\": \"image-container\"},\n",
    "            {\"name\": \"figure\"}\n",
    "        ]\n",
    "    filtered = []\n",
    "    for block in blocks:\n",
    "        split_items = split_nested_blocks(block, target_tags)\n",
    "        soup = BeautifulSoup(block, \"html.parser\")\n",
    "        # Gather all non-whitespace descendants\n",
    "        descendants = [el for el in soup.descendants if getattr(el, 'name', None)]\n",
    "        # Gather all target elements\n",
    "        target_elements = []\n",
    "        for target in target_tags:\n",
    "            target_elements.extend(soup.find_all(target[\"name\"], class_=target.get(\"class_\")))\n",
    "        # If all non-whitespace descendants are target elements, keep only split_items\n",
    "        if descendants and set(descendants) == set(target_elements):\n",
    "            filtered.extend(split_items)\n",
    "        else:\n",
    "            filtered.append(block.strip())\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab211949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract story data\n",
    "classic_story_settings = classic_item_data[\"values\"][\"settings\"]\n",
    "classic_story_theme = classic_story_settings[\"theme\"]\n",
    "classic_story_title = classic_item_data[\"values\"][\"title\"]\n",
    "classic_story_data = classic_item_data[\"values\"][\"story\"]\n",
    "\n",
    "# Extract tabs (entries list)\n",
    "entries = classic_story_data[\"entries\"]\n",
    "\n",
    "# Fetch theme group\n",
    "classic_theme_group = classic_story_theme[\"colors\"][\"group\"]\n",
    "if classic_theme_group == \"dark\":\n",
    "    new_theme = Themes.OBSIDIAN\n",
    "elif classic_theme_group == \"light\":\n",
    "    new_theme = Themes.SUMMIT\n",
    "\n",
    "created_storymaps = []\n",
    "# loop_limit = 0 # Zero indexed. For testing/debugging only\n",
    "target_index = 1  # Change to the index of the entry you want to process (0-based)\n",
    "for i, entry in enumerate(entries):\n",
    "    if i != target_index:\n",
    "        continue # Skip all except the target index\n",
    "    # Create a new StoryMap\n",
    "    story = StoryMap()\n",
    "    story.theme(new_theme)\n",
    "\n",
    "    # Create Sidecar immersive section\n",
    "    sidecar = Sidecar(style=\"docked-panel\")\n",
    "\n",
    "    # Add Sidecar to story\n",
    "    story.add(sidecar)\n",
    "\n",
    "    # Determine media content for main stage\n",
    "    media_info = entry.get(\"media\", {})\n",
    "    media_type = media_info.get(\"type\")\n",
    "\n",
    "    media_content = None\n",
    "    if media_type == \"webmap\":\n",
    "        webmap_id = media_info.get('webmap', {}).get('id')\n",
    "        if webmap_id:\n",
    "            media_content = Map(webmap_id)\n",
    "    elif media_type == \"webpage\":\n",
    "        webpage_url = media_info.get(\"webpage\", {}).get(\"url\")\n",
    "        if webpage_url:\n",
    "            media_content = Embed(webpage_url)\n",
    "\n",
    "    # Fetch content from description (HTML)\n",
    "    description_html = entry.get(\"description\", \"\")\n",
    "\n",
    "    content_blocks = extract_deepest_relevant_blocks(description_html)\n",
    "\n",
    "    # content_blocks_including_nested = extract_content_blocks(description_html)\n",
    "    # content_blocks = filter_parent_blocks(content_blocks_including_nested)\n",
    "    # content_blocks_strict = filter_parent_blocks_strict(content_blocks_including_nested)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e29bb930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<div class=\"image-container\">\\n<div class=\"image-container\">\\n<p style=\"text-align:center\"><img alt=\"\" height=\"240\" src=\"https://lh3.googleusercontent.com/-OZs54tCn6mM/VdYjIf-XQ0I/AAAAAAAAAOU/HrFElBu60xw/s1600/legends-08.png\" width=\"496\"/></p>\\n</div>\\n\\xa0\\n\\n<div class=\"image-container\">\\xa0</div>\\n<div class=\"image-container\"><span style=\"color:#E5FA84\"><span style=\"font-size:20px\"><strong>Hurricane Katrina displaced over one million Louisiana residents -- an estimated 277,000 did not come back to resettle.</strong>\\xa0</span></span></div>\\n</div>', '<p style=\"text-align:center\"><img alt=\"\" height=\"132\" src=\"https://lh3.googleusercontent.com/-Frhyo5iY8mo/Vc40Ea084sI/AAAAAAAAAEY/r_AIGqcYI9E/s1600/legends-03.png\" width=\"402\"/></p>', '<p>\\xa0</p>', '<div>\\n<p><span style=\"font-size:12px\">Threatened by one of the most destructive and influential storms in United States history, those living in the path of Hurricane Katrina\\xa0fled to every state in the country, with many\\xa0unable to return home to Louisiana after the storm left.\\xa0This map, which uses data from the 2006 American Community Survey,\\xa0presents a good, but imperfect illustration of\\xa0where evacuees ended up by showing\\xa0where\\xa0residents who lived in Louisiana\\xa0in 2005 moved to as of 2006.\\xa0<span style=\"color:#E2F782\"><strong>Click on each state</strong></span>\\xa0for details.\\xa0</span></p>\\n<p>\\xa0</p>\\n<p><span style=\"font-size:10px\">Data Sources:\\xa0<a href=\"https://www.census.gov/hhes/migration/data/acs/state-to-state.html\" target=\"_blank\">2006 American Community Survey 1-year Estimates, State-to-State Migration Flows</a>,\\xa0<a href=\"http://www.nhc.noaa.gov/\" target=\"_blank\">NHC</a>,\\xa0<a href=\"http://www.noaa.gov/\" target=\"_blank\">NOAA</a>, <a href=\"https://www.weather.gov/\" target=\"_blank\">NWS</a>.\\xa0</span></p>\\n<p>\\xa0</p>\\n</div>']\n"
     ]
    }
   ],
   "source": [
    "print(content_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05736a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode for traversing html tree\n",
    "# get all leaf nodes\n",
    "# if node is a leaf node (no children)\n",
    "#     recursively check its parents until the parent is equal to the root node.\n",
    "#     step down one level\n",
    "#     check for img, video, audio, iframe, embed, map tags\n",
    "#     if no other tags found, mark as text node\n",
    "#     if img, video, audio, iframe, embed, map tag found mark as that type\n",
    "#     traverse up tree until a <p>, <div> or <span>is found\n",
    "#     once a <p> is found, check if its parent is a <div class=image-container>, if so mark as Image()\n",
    "#     once a <div> is found, check if its parent is a <Figure class=caption>, if so process as Image() with caption\n",
    "#     once a <span> is found, check if its parent is a <span>\n",
    "#     otherwise, capture all descendant tags and process the node as its marked type (Text | Image | Video | Audio | Embed | Map | Button | Code | Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b698bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(description_html, \"html.parser\")\n",
    "soup_content = list(soup.children)\n",
    "# soup_content_collapsed = [child for child in soup_content if getattr(child, 'name', None)]\n",
    "# for el in soup_content_collapsed[0]:\n",
    "#     el_nodes = []\n",
    "#     for tag in el.find_all(True):  # All tags, nested included\n",
    "#         # A leaf node has no child tags\n",
    "#         if not tag.find_all(True):\n",
    "#             el_nodes.append(tag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
